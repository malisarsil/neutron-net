# Usage
The system consists of 8 files. To operate the system as a whole, only `pipeline.py` needs to be run. If you wish to use the other files independently of one another this is also possible; please read the individual sections for each component if this is the case.

## The Pipeline
If you have not trained any models, you can set up the pipeline by calling `Pipeline.setup` which will, by default, generate reflectivity data, convert to images, train a classifier and train regressors using the data. You will need to provide the layers for which you wish to set the pipeline up for, e.g. [1,2,3] for up to 3-layer structures. You can also modify the number of curves generated (which will significantly impact runtime), chunk size for h5 storage, and the number of epochs to train the classifier and regressors for. You can also specify whether you want to use a neutron or x-ray probe and whether you want to apply noise to the data with the `xray` and `noisy` flags respecitvely. If you have already generated data and converted it to images, you can set the `generate_data` flag to false to train on your own data. Likewise, if you already have trained a classifier, this can be loaded by setting the `train_classifier` flag to false. Finally, if you have already trained the regressors for each layer, you can load these instead of training from scratch by setting the `train_regressor` flag to false.

To run the pipeline, you can call the `Pipeline.run` method. You must provide a path to a directory containing .dat files to predict on (these should be CSV files of the form X, Y, Error). Please remove any points with zeros as these will cause issues when calculating the log-likelihood when fitting with refnx. You will also need to provide the file paths of a trained classifier as well as a dictionary of file paths for trained regressors for each layer you wish to classify. By running the pipeline, the provided classifier will be used for a layer prediction. This will then be used to determine the regressor to use when predicting each layer's SLD and thickness using dropout prediction. These predictions are then fed into refnx models and plotted against the given data. Fitting can optionally be applied by setting the `fit` parameter to True; the predictions are used as initial estimates for the fitting algorithm for each of the models.

## Synthetic Data Generation
Data can be generated with refnx using the `generate_refnx.py` file. Specifically, `NeutronGenerator.generate` allows for generation of `n` refnx Structure objects with a specified number of layers, SLDs and thicknesses within given bounds using a neutron probe. The code is currently setup to use a silicon substrate with 2.047 Å SLD. A default 2 Å roughness between layers is included to simulate real data. Thicknesses choices are biased towards tinner layers. To generate with an x-ray probe, you can use `XRayGenerator.generate`

The `NeutronGenerator.save` or `XRayGenerator.save` method can then be used to store these `n` structures as reflectivity curves in h5 format. The min, max and number of momentum transfer values can be specified along with background, scale and resolution parameters (defaults are 0, 1 and 2 respectively). The option to add sample and background noise is also available with the noisy flag. This requires the included directbeam_noise.dat sample.

## Creating Images
Images are required as input to the classifier and regressors and `generate_data.py` facilitates creation of these images. Data is split into training, validation and test splits for use in these models.

The `generate_images` function can be called with the file path to a directory containing h5 files (in the correct format) to generate images for. Also provided to this function is a list of the layers for which images are to be generated for the corresponding file. For example, passing [1,2,3] will create images for any files containing 'one', 'two' or 'three' in their file name in the given data path. These will then be saved together in the given save path directory. This allows for files with curves of a specific layer to be created, as is required for regression, as well as for files containing curves of multiple different layers, as is required for classification.

Depending on the number of curves being generated, the chunk size for the h5 files can be modified for potential speedup. Please note that this process can take some time with large numbers of curves and will potentially generate large files (~6GB for 50,000 curves). During creation of these images of the input reflectivity curves, targets are scaled to be between 0 and 1 for speeding up training and the data is shuffled.

## Merging Files
To merge train.h5, validate.h5 and test.h5 files, you can call the `merge` function in `merge_data.py`. After creating separate h5 files for each layer for each of the regressors, these files can be merged together to get a combined file for use in training the classifier. This allows for reuse of data in the classifier as well as the regressors and also allows for images to be generated on separate machines before being combined.

## Classification
To perform classification, call the `classify` function in `classification.py`. You will need to provide a file path to the data (train.h5, validate.h5 and test.h5) to test and/or train on. Optionally, a save path can be provided to save a newly trained model to. A load path could also be provided to load an existing classifier for further training and evaluation; the train flag indicates whether to perform training or not. Hyperparameters that can be adjusted include the number of epochs to train for, the learning rate, batch size and dropout rate. The classifier will work with up to 3-layer classification.

The `show_plots` option for the `classify` function can be set to True to generate a confusion matrix when evaluating against a test set. This calls the `ConfusionMatrixPrinter.pretty_plot` method in `confusion_matrix_pretty_print.py`. The confusion matrix is the result of predicting on the given test set and then displaying how each individual example was classified, against the ground truth label.

## Regression
To perform regression, call the `regress` function in `regression.py`. You will need to provide a file path to the data containing data for a specific layer (train.h5, validate.h5 and test.h5) to test and/or train on. You must also provide the layer for which the regressor is being used for. Optionally, a save path can be provided to save a newly trained model to. A load path could also be provided to load an existing regressor for further training and evaluation; the train flag indicates whether to perform training or not. Hyperparameters that can be adjusted include the number of epochs to train for, the learning rate, batch size and dropout rate.

The `show_plots` option for the `regress` function can be set to True to generate a prediction plot when evaluating against a test set. The plot is of the predicted depths and SLDs against ground truth values for each layer.

## Plotting with Error Bars
To generate the plots used in the paper of predictions against ground truths with error bars, you will need to run the code in `plotting.py`. The `Plotter.kpd_plot` method creates the plot using the `KerasDropoutPredicter` (KDP) class. The KDP takes trained models and uses dropout at test time to make Bayesian-like predictions. The `n_iter` parameter controls the number of predictions that are made for each example. The results are "unscaled", and then mean and standard deviation are taken; these form the predictions and associated errors repetitively.